{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3:58 form work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14Oct at 3:45 test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "``# Supervised Capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we will use Houston, Texas, real estate listing data showing which properties are active for sale and which are sold or terminated. We try here to predict those terminated ones. Prior to the prediction, we will perform many cleaning steps and some feature engineering. Later we will use regression models to predict the listing price. This prediction will help to determine if a listing is over- or underpriced. Susch deviation from a correct market price may later explain why the listing get sold or terminated. And to predict this deviation we will use classification methods to see if we can predict an active listing will get terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.patches import PathPatch\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "from numpy.random import seed\n",
    "import os\n",
    "#from path import Path\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from scipy.stats import bartlett\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats import jarque_bera\n",
    "from scipy.stats import levene\n",
    "from scipy.stats import normaltest\n",
    "import scipy.stats as stats\n",
    "from scipy.stats.mstats import winsorize\n",
    "from scipy.stats import zscore\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "from sklearn import decomposition\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, Lasso, LassoCV, RidgeCV, Ridge\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#from sklearn.model_selection import \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.eval_measures import mse, rmse\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100) \n",
    "pd.set_option('display.max_colwidth', -1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# DO NOT DELETE ############################################# \n",
    "\n",
    "# Help functions to gather basic descriptions\n",
    "def describe(df):\n",
    "    return pd.concat([df.describe().T,\n",
    "                      df.mad().rename('mean abs dev'),\n",
    "                      df.skew().rename('skew'),\n",
    "                      df.kurt().rename('kurt'),\n",
    "                      df.nunique().rename('unique')\n",
    "                     ], axis=1).T\n",
    "\n",
    "def data_type_summary(df):\n",
    "    # get Object data type summary\n",
    "    df_stat_object = pd.DataFrame([])\n",
    "    df_stat_object = df.describe(include = ['O'])\n",
    "    df_stat_object.loc['dtype'] = df.dtypes\n",
    "    df_stat_object.loc['size'] = len(df)\n",
    "    #df_stat_object.loc['% null'] = df.isnull().count().round(2)\n",
    "    #df_stat_object.loc['% null'] = ((df0.isnull().sum()/df0.shape[0])*100).round(3)\n",
    "    \n",
    "    \n",
    "    # get numerical data type summary\n",
    "    df_stat_num = pd.DataFrame([])\n",
    "    df_stat_num = df.describe(include = [np.number])\n",
    "    df_stat_num.loc['dtype'] = df.dtypes\n",
    "    df_stat_num.loc['size'] = len(df)\n",
    "    #df_stat_object.loc['% null'] = df.isnull().count().round(2)\n",
    "    df_stat_object.loc['% null'] = ((df0.isnull().sum()/df0.shape[0])*100).round(3)\n",
    "    \n",
    "    # get date data type summary\n",
    "    df_stat_date = df.describe(include = ['datetime64']) \n",
    "    df_stat_date.loc['dtype'] = df.dtypes\n",
    "    df_stat_date.loc['size'] = len(df)\n",
    "    #df_stat_object.loc['% null'] = df.isnull().count().round(2)\n",
    "    #df_stat_object.loc['% null'] = ((df0.isnull().sum()/df0.shape[0])*100).round(3)\n",
    "    \n",
    "    # Combine all\n",
    "    result = pd.concat([df_stat_num, df_stat_object, df_stat_date ], axis=1, sort=False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# DO NOT DELETE ############################################# \n",
    "\n",
    "def describe_with_nulls(df):\n",
    "    nan_cols = [i for i in df.columns if df[i].isnull().any()]\n",
    "    # get Object data type summary\n",
    "    df_stat_num = pd.DataFrame([])\n",
    "    df_stat_num = df0[nan_cols].describe(include = [np.number, 'O', 'datetime64']) \n",
    "    df_stat_num.loc['dtype'] = df.dtypes\n",
    "    df_stat_num.loc['size'] = len(df)\n",
    "    #df_stat_num.loc['% null'] = df.isnull().count().round(2)\n",
    "    df_stat_num.loc['% null'] = ((df0.isnull().sum()/df0.shape[0])*100).round(3)\n",
    "    return df_stat_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# DO NOT DELETE ############################################# \n",
    "# Function to produce statistics from scikit learn regression estimators\n",
    "def regression_results(y_true, y_pred):\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# DO NOT DELETE ############################################# \n",
    "# Function to move specific column to the left side for easier view\n",
    "def move_to_left(df, column_name):\n",
    "    df= df[ [str(column_name)] + [ col for col in df.columns if col != str(column_name) ] ]\n",
    "    return df\n",
    "\n",
    "def move_to_left_get_dup(df, column_name):\n",
    "    df = df[ [str(column_name)] + [ col for col in df.columns if col != str(column_name) ] ]\n",
    "    df = df[df.duplicated(str(column_name))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import_cols = ['MLS Number', 'Property Type', 'Status', 'Street Number', 'Street Name', 'Unit Number', 'City/Location' \\\n",
    "                  , 'Zip Code', 'County', 'List Price', 'Close Price', 'Close Date', 'Area', 'Subdivision'\\\n",
    "                  , 'Master Planned Community', 'Market Area', 'School District', 'School Elementary', 'School Middle' \\\n",
    "                  , 'School High', 'Building SqFt','Price Sq Ft List', 'Price Sq Ft Sold', 'Lot Size', 'Acres' \\\n",
    "                  , 'Price Acre List', 'Price Acre Sales', 'Year Built', 'Bedrooms','Baths Full', 'Baths Half' \\\n",
    "                  , 'Baths Total', 'Room Count', 'Fireplaces Number', 'Stories', 'New Construction', 'New Construction Desc' \\\n",
    "                  , 'Pool Private', 'No Of Garage Cap', 'Style', 'DOM', 'CDOM', 'List Office MLSID', 'List Office Name' \\\n",
    "                  , 'List Agent MLSID', 'List Agent Full Name', 'Selling Office MLSID', 'Selling Office Name', 'Selling Agent MLSID' \\\n",
    "                  , 'Selling Agent Full Name', 'Realist Owner Name', 'Last Change Timestamp', 'List Date ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['.git',\n",
       " 'readme.md',\n",
       " 'Supervised_Capstone.ipynb',\n",
       " 'Supervised_Capstone1.ipynb']"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Analytics\\\\HAR Pipeline\\\\Step0_Raw\\\\Step0_Data\\\\'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-f082e7702f0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpath1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:\\Analytics\\\\\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpath2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'HAR Pipeline\\Step0_Raw\\Step0_Data\\\\'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mpath2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdf_import\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Analytics\\\\HAR Pipeline\\\\Step0_Raw\\\\Step0_Data\\\\'"
     ]
    }
   ],
   "source": [
    "#path1 = r'C:\\Users\\User\\OneDrive - Seedergy\\MAEN\\Code\\Analytics\\\\'\n",
    "#path1 = 'G:\\Amir\\Analytics\\\\'\n",
    "path1 = \"C:\\Analytics\\\\\"\n",
    "path2 = 'HAR Pipeline\\Step0_Raw\\Step0_Data\\\\'\n",
    "files = os.listdir(path1+path2)\n",
    "\n",
    "df_import = pd.DataFrame()\n",
    "for f in files:\n",
    "    data = pd.read_excel(path1+f, sheet_name=0, header= None, skiprows=1, names=df_import_cols)\n",
    "    df_import = df_import.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df_import.copy()\n",
    "# Add a unique ID for future reference\n",
    "df0['import_id'] = [i for i in range(len(df0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup columns headings\n",
    "df0.columns = df0.columns.str.lstrip()\n",
    "df0.columns = df0.columns.str.rstrip()\n",
    "df0.columns = df0.columns.str.replace(' ', '_')\n",
    "df0.columns = df0.columns.str.lower()\n",
    "# Make the whole dataframe to lower case\n",
    "df0 = df0.applymap(lambda s:s.lower() if type(s) == str else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the numerical values from string\n",
    "df0['school_district_num'] = df0['school_district'].str.extract('(\\d+)').astype(str)\n",
    "df0['school_district_num'] = df0['school_district_num'].astype(str).astype(float) \n",
    "# Change type to date time\n",
    "df0[\"last_change_timestamp\"]  = pd.to_datetime(df0[\"last_change_timestamp\"])\n",
    "df0[\"list_date\"]  = pd.to_datetime(df0[\"list_date\"])\n",
    "df0[\"close_date\"]  = pd.to_datetime(df0[\"close_date\"])\n",
    "df0[\"year_built\"]  = pd.to_datetime(df0[\"year_built\"])\n",
    "# Change currency type to float\n",
    "currency_cols = ['list_price', 'close_price']\n",
    "df0[currency_cols] = (df0[currency_cols].replace( '[\\$,)]','', regex=True ).replace( '[(]','-',   regex=True ).astype(float))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a new coutnt of shape\n",
    "print('The shapes of dataset as we proceed: {}, {}'.format(df_import.shape, df0.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with many missing values\n",
    "df0 = df0.drop(['unit_number','close_date', 'master_planned_community','new_construction_desc','realist_owner_name','mls_number' ], axis=1)\n",
    "# Drop calculated values as these can be recalcualted when we fill the missing values\n",
    "df0 = df0.drop(['fireplaces_number', 'price_sq_ft_list', 'price_sq_ft_sold','price_acre_sales', 'acres', 'price_acre_list' ], axis=1)\n",
    "df0 = df0.rename(columns={'city/location':'city_location'}) #, inplace=True)\n",
    "\n",
    "# Drop these as they influence the target and are irrelevant\n",
    "df0 = df0.drop(['selling_office_mlsid', 'selling_office_name', 'selling_agent_mlsid', 'selling_agent_full_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a new coutnt of shape\n",
    "print('The shapes of dataset as we proceed: {}, {}'.format(df_import.shape, df0.shape))"
   ]
  },
  {
   "source": [
    "## Fill missing values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nulls of categorical data with a place holder instead of removing them so not to reduce the frequency of the target variable\n",
    "df0['lot_size'].fillna(df0['lot_size'].median(), inplace=True)\n",
    "df0['area'].fillna(\"value missing\", inplace=True)\n",
    "df0['street_number'].fillna(\"value missing\", inplace=True)\n",
    "df0['subdivision'].fillna(\"value missing\", inplace=True)\n",
    "df0['market_area'].fillna(\"value missing\", inplace=True)\n",
    "df0['school_elementary'].fillna(\"value missing\", inplace=True)\n",
    "df0['school_middle'].fillna(\"value missing\", inplace=True)\n",
    "df0['school_high'].fillna(\"value missing\", inplace=True)\n",
    "df0['room_count'].fillna(df0['room_count'].median(), inplace=True)\n",
    "df0['building_sqft'].fillna(df0['building_sqft'].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a new coutnt of shape\n",
    "print('The shapes of dataset as we proceed: {}, {}'.format(df_import.shape, df0.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a dict for min and max normal values for each column. Drop rows with values outside of this range \n",
    "value_range = {\n",
    "    'building_sqft': (800, 5000),\n",
    "    'lot_size': (3000, 30000),\n",
    "    'bedrooms': (1,8),\n",
    "    'baths_full': (1, 5),\n",
    "    'baths_half': (0, 5),\n",
    "    'baths_total': (1, 10),\n",
    "    'room_count': (1, 10),\n",
    "    'stories': (1,2),\n",
    "    'no_of_garage_cap': (0,3),\n",
    "    'dom': (0, 600),\n",
    "    'cdom': (0, 600)}\n",
    "\n",
    "def drop_extreme_rows(df):\n",
    "    for c in range(0, len(list(value_range.keys()))):\n",
    "        col_name = list(value_range.keys())[c]\n",
    "        col_min_val = list(value_range.values())[c][0]\n",
    "        col_max_val = list(value_range.values())[c][1]\n",
    "        df = df[df[col_name].between(col_min_val, col_max_val)]\n",
    "    return df\n",
    "df0 = drop_extreme_rows(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a new coutnt of shape\n",
    "print('The shapes of dataset as we proceed: {}, {}'.format(df_import.shape, df0.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df0.copy()"
   ]
  },
  {
   "source": [
    "## Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add Target\n",
    "df1['target'] = np.where((df1.status == \"expired\") | (df1.status == \"terminated\") | (df1.status == \"withdrawn\"), 1,0)\n",
    "# Reorder the columns so those important one are shown on the left side\n",
    "df1 = df1[ ['target'] + [ col for col in df1.columns if col != 'target' ] ]\n",
    "# Slice the data into more comparable property types\n",
    "df1 = df1[df1['property_type']=='single-family']\n",
    "print('The shapes of dataset as we proceed: {}, {}, {}'.format(df_import.shape, df0.shape, df1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with ones\n",
    "df1['ones'] = 1\n",
    "# Create the ratio of lot size as of building size for filling the nans lalter\n",
    "df1['lot_build_ratio'] = df1['lot_size'].div(df1['building_sqft'].where(df1['lot_size'] != 0, 0 ))#np.nan))\n",
    "# Get the median of lot size to building size ratio\n",
    "lot_build_ratio_median = df1.loc[df1['lot_build_ratio'] !='NaN', 'lot_build_ratio'].median()\n",
    "df1[\"lot_build_ratio\"] = np.where(lot_build_ratio_median =='NaN', lot_build_ratio_median, lot_build_ratio_median)\n",
    "# Get a new coutnt of shape\n",
    "print('The shapes of dataset as we proceed: {}, {}, {}'.format(df_import.shape, df0.shape, df1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the median of dates\n",
    "year_built_median = df1['year_built'].astype('datetime64[ns]').quantile(.5)\n",
    "list_date_median = df1['list_date'].astype('datetime64[ns]').quantile(.5)\n",
    "# Fill dates with null\n",
    "df1['year_built'].fillna(year_built_median, inplace=True)\n",
    "df1['list_date'].fillna(list_date_median, inplace=True)\n",
    "# Get a new coutnt of shape\n",
    "print('The shapes of dataset as we proceed: {}, {}, {}'.format(df_import.shape, df0.shape, df1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 and df3 are kept unused on purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if the unique index is duplicated. A duplication cause problem later\n",
    "move_to_left_get_dup(df0, 'import_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add price boundry\n",
    "lower_bound = 200000\n",
    "upper_bound = 750000\n",
    "df4 = df4[((df4['list_price'] >= lower_bound) & (df4['list_price'] <=upper_bound))]\n",
    "\n",
    "# Get a new coutnt of shape\n",
    "print('The shapes of dataset as we proceed: {}, {}, {}'.format(df0.shape, df1.shape, df4.shape))"
   ]
  },
  {
   "source": [
    "## Add rank for the target"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add performance rank to identify top producers\n",
    "df4['target_sum'] = df4.groupby('list_office_mlsid')['target'].transform('sum')\n",
    "df4['target_rank'] = df4['target_sum'].rank(method='dense', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df4.hist(column='target_rank', bins=10, grid=False, figsize=(10,8), color='#86bf91', zorder=2, rwidth=0.9)\n",
    "\n",
    "ax = ax[0]\n",
    "for x in ax:\n",
    "\n",
    "    # Despine\n",
    "    x.spines['right'].set_visible(False)\n",
    "    x.spines['top'].set_visible(False)\n",
    "    x.spines['left'].set_visible(False)\n",
    "\n",
    "    # Switch off ticks\n",
    "    x.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "\n",
    "    # Draw horizontal axis lines\n",
    "    vals = x.get_yticks()\n",
    "    for tick in vals:\n",
    "        x.axhline(y=tick, linestyle='dashed', alpha=0.4, color='#eeeeee', zorder=1)\n",
    "\n",
    "    # Remove title\n",
    "    x.set_title(\"Target rank of Listing Agent \")"
   ]
  },
  {
   "source": [
    "## See Listing Price vs. Size (Sqft)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare the data for plotting\n",
    "# separate x and y\n",
    "x = df4[\"list_price\"]\n",
    "y = df4[\"building_sqft\"]\n",
    "\n",
    "# instanciate the figure\n",
    "fig = plt.figure(figsize = (12, 6))\n",
    "# in this case we use gridspec.\n",
    "gs = fig.add_gridspec(5, 5)\n",
    "ax1 = fig.add_subplot(gs[:4, :-1])\n",
    "\n",
    "# plot the data\n",
    "# main axis: scatter plot\n",
    "ax1.scatter(x, y) #, c = df4.target.astype('category').cat.codes) \n",
    "\n",
    "# set the labels for x and y\n",
    "ax1.set_xlabel(\"Building, sqf\")\n",
    "ax1.set_ylabel(\"Listing Price, k$\")\n",
    "\n",
    "# set the title for the main plot\n",
    "ax1.set_title(\"Building Size vs Listin Price\")\n",
    "\n",
    "# prettify the plot\n",
    "# get rid of some of the spines to make the plot nicer\n",
    "ax1.spines[\"right\"].set_color(\"None\")\n",
    "ax1.spines[\"top\"].set_color(\"None\")\n",
    "\n",
    "# using familiar slicing, get the bottom axes and plot\n",
    "ax2 = fig.add_subplot(gs[4:, :-1])\n",
    "ax2.hist(x, 40, orientation = 'vertical', color = \"pink\")\n",
    "\n",
    "# invert the axis (it looks up side down)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# prettify the plot\n",
    "# set the ticks to null\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "# no axis to make plot nicer\n",
    "ax2.axison = False\n",
    "\n",
    "# using familiar slicing, get the left axes and plot\n",
    "ax3 = fig.add_subplot(gs[:4, -1])\n",
    "ax3.hist(y, 40, orientation = \"horizontal\", color = \"pink\")\n",
    "\n",
    "# prettify the plot\n",
    "# set the ticks to null\n",
    "ax3.set_xticks([])\n",
    "ax3.set_yticks([])\n",
    "# no axis to make plot nicer\n",
    "ax3.axison = False\n",
    "\n",
    "# make all the figures look nicier\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target vs. Building & Lot Size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciate the figure\n",
    "fig = plt.figure(figsize = (12, 6))\n",
    "ax = fig.add_subplot(1,1,1,)\n",
    "\n",
    "# iterate over each category and plot the data. This way, every group has it's own color. Otherwise everything would be blue\n",
    "for cat in sorted(list(df4[\"target\"].unique())):\n",
    "# filter x and the y for each category\n",
    "    lot = df4[df4[\"target\"] == cat][\"lot_size\"]\n",
    "    build = df4[df4[\"target\"] == cat][\"building_sqft\"]\n",
    "    # plot the data\n",
    "    ax.scatter(lot, build, label = cat, s = 40)\n",
    "    \n",
    "# prettify the plot\n",
    "\n",
    "# eliminate 2/4 spines (lines that make the box/axes) to make it more pleasant\n",
    "ax.spines[\"top\"].set_color(\"None\") \n",
    "ax.spines[\"right\"].set_color(\"None\")\n",
    "\n",
    "# set a specific label for each axis\n",
    "ax.set_xlabel(\"Lot Size, sqf\") \n",
    "ax.set_ylabel(\"Building, sqf\")\n",
    "\n",
    "# change the lower limit of the plot, this will allow us to see the legend on the left\n",
    "ax.set_xlim(-0.01) \n",
    "ax.set_title(\"Lost Size vs. Building Size: Target\")\n",
    "ax.legend(loc = \"upper left\", fontsize = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual of all numeric features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all numerica columns\n",
    "numerics = ['int_', 'int8', 'uint8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "df4_num = df4.select_dtypes(include=numerics)\n",
    "num_cols = df4_num.columns\n",
    "# Plot all of them\n",
    "df4[num_cols].plot(subplots=True, layout=(8, 5), figsize=(30, 20), sharex=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Price vs Close Price: Size, Mean, Median, Max and Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot1 = pd.pivot_table(df4,index=[\"target\"],values=[\"list_price\", \"close_price\"], aggfunc=[len, np.mean, np.median, np.max, np.min], margins=True).round(0)\n",
    "\n",
    "# function to pivot showing in 1000$\n",
    "def format(x):\n",
    "        return \"${:.1f}K\".format(x/1000)\n",
    "\n",
    "# Apply function to specifc columns\n",
    "\n",
    "lst = pivot1.columns[2:10]\n",
    "for l in lst:\n",
    "    pivot1[l] = pivot1[l].apply(format)\n",
    "pivot1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building size vs List Price: Size, Mean, Median, Max and Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot_df = df0.loc[df0['property_type'].isin(['single-family', 'townhouse/condo', 'mid/hi-rise condo'])]\n",
    "pivot2 = pd.pivot_table(df4,index=[\"target\"],values=[\"building_sqft\", \"list_price\"], aggfunc=[len, np.mean, np.median, np.max, np.min], margins=True).round(0)\n",
    "\n",
    "# function to pivot showing in 1000$\n",
    "def format(x):\n",
    "        return \"{:.1f}K\".format(x/1000)\n",
    "\n",
    "# Apply function to specifc columns\n",
    "\n",
    "lst = pivot2.columns[2:10]\n",
    "for l in lst:\n",
    "    pivot2[l] = pivot2[l].apply(format)\n",
    "pivot2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and fix any outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all numerica columns\n",
    "numerics = ['int_', 'int8', 'uint8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "df4_num = df4.select_dtypes(include=numerics)\n",
    "col_names = df4_num.columns\n",
    "\n",
    "# Eliminate unwanted elements\n",
    "unwanted = {'close_price', 'new_construction', 'pool_private', 'ones', 'target', 'target_sum', 'target_rank'}\n",
    "col_names = [ele for ele in col_names if ele not in unwanted]\n",
    "                      \n",
    "# Get a dict of numerial columns\n",
    "col_dict = {c: i for i, c in enumerate(col_names, start=1)}\n",
    "\n",
    "# Detect outliers in each variable using box plots.\n",
    "plt.figure(figsize=(20,70))\n",
    "\n",
    "for variable,i in col_dict.items():\n",
    "                     plt.subplot(30,3,i)\n",
    "                     plt.boxplot(df4[variable],whis=1.5)\n",
    "                     plt.title(variable)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate number of outliers and its percentage in each variable using Tukey's method.\n",
    "for variable in col_dict.keys():\n",
    "    q75, q25 = np.percentile(df4[variable], [75 ,25])\n",
    "    iqr = q75 - q25\n",
    "    min_val = q25 - (iqr*1.5)\n",
    "    max_val = q75 + (iqr*1.5)\n",
    "    col_length = len(np.where(df4[variable])[0])\n",
    "    print(\"Number of outliers and percentage of it in {} : {} and {}\".format(variable,\n",
    "                                                len((np.where((df4[variable] > max_val) | (df4[variable] < min_val))[0])),\n",
    "                                                len((np.where((df4[variable] > max_val) | (df4[variable] < min_val))[0]))*100/col_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shapes of dataset as we proceed: {}'.format(df4.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if the unique index is duplicated. A duplication cause problem later\n",
    "move_to_left_get_dup(df4, 'import_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winsorize outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all numerica columns\n",
    "numerics = ['int_', 'int8', 'uint8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "df4_num = df4.select_dtypes(include=numerics)\n",
    "col_names = df4_num.columns\n",
    "\n",
    "# Eliminate unwanted elements\n",
    "unwanted = {'close_price', 'new_construction', 'pool_private', 'ones', 'target', 'target_sum', 'target_rank'}\n",
    "col_names = [ele for ele in col_names if ele not in unwanted]\n",
    "\n",
    "# Define Winsorize funtion\n",
    "def using_mstats(s):\n",
    "    return winsorize(s, limits=[0.01, 0.01])\n",
    "\n",
    "\n",
    "# Apply the function\n",
    "#df4[col_names] = df4[col_names].apply(lambda x: winsorize(x, limits=[0.01, 0.01]))\n",
    "\n",
    "limit = 0.1\n",
    "df4['no_of_garage_cap'] = winsorize(df4['no_of_garage_cap'], limits=limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shapes of dataset as we proceed: {}'.format(df4.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_to_left_get_dup(df4, 'import_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy to new dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df5 = df4.copy()\n",
    "print('The shapes of dataset as we proceed: {}, {}, {}, {}'.format(df0.shape, df1.shape, df4.shape, df5.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform simple regression for predicting the listing price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_with_nulls(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Y is the target variable\n",
    "Y = df5['list_price']\n",
    "# X is the feature set which includes\n",
    "numerics = ['int_', 'int8', 'uint8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "print(numerics)\n",
    "df5_num = df5.select_dtypes(include=numerics)\n",
    "print(df5_num.shape)\n",
    "X = df5_num.drop(['list_price', 'close_price', 'target'], axis=1)\n",
    "\n",
    "\n",
    "# We create a LinearRegression model object\n",
    "# from scikit-learn's linear_model module.\n",
    "lrm = linear_model.LinearRegression()\n",
    "\n",
    "# fit method estimates the coefficients using OLS\n",
    "lrm.fit(X, Y)\n",
    "\n",
    "# Inspect the results.\n",
    "print('\\nCoefficients: \\n', lrm.coef_)\n",
    "print('\\nIntercept: \\n', lrm.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gauss-Markov Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## According to the Gauss–Markov theorem, in a linear regression model the ordinary least squares (OLS) estimator gives the best linear unbiased estimator (BLUE) of the coefficients, provided that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target variable should be a linear function of the model's coefficients\n",
    "### The expectation of errors (residuals) is 0\n",
    "### The errors have equal variance — homoscedasticity of errors\n",
    "### Features should have low multicollinearity\n",
    "### The errors are uncorrelated\n",
    "### The explanatory variables and errors should be independent (exogeneity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The way we have specified our model here supports the linearity assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check assumption that error term should be zero on average¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = lrm.predict(X)\n",
    "errors = Y - predictions\n",
    "\n",
    "print(\"Mean of the errors in the target model is: {}\".format(np.mean(errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we have a constant term in the model, the average of the model's error is effectively zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for the homoscedasticity assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A model is homoscedastic when the distribution of its error terms (known as \"scedasticity\") is consistent for all predicted values. In other words, the error variance shouldn't systematically change across observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(predictions, errors)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual')\n",
    "plt.axhline(y=0)\n",
    "plt.title('Residual vs. Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that our error terms are not consistently distributed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are several formal statistical tests that we can use to determine whether there is heteroscedasticity in the error terms. Two of these are Barlett and Levene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Barlett and Levene tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import bartlett\n",
    "from scipy.stats import levene\n",
    "\n",
    "bart_stats = bartlett(predictions, errors)\n",
    "lev_stats = levene(predictions, errors)\n",
    "\n",
    "print(\"Bartlett test statistic value is {0:3g} and p value is {1:.3g}\".format(bart_stats[0], bart_stats[1]))\n",
    "print(\"Levene test statistic value is {0:3g} and p value is {1:.3g}\".format(lev_stats[0], lev_stats[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The p-values of both tests is meaning we can reject the null hypothesis which again means our errors are hetetroscedastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for low multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual features should be only weakly correlated with one another, and ideally completely uncorrelated. When features are correlated, they may both explain the same pattern of variance in the outcome. The model will attempt to find a solution, potentially by attributing half the explanatory power to one feature and half to the other. This isn’t a problem if our only goal is prediction, because then all that matters is that the variance gets explained. However, if we want to know which features matter most when predicting an outcome, multicollinearity can cause us to underestimate the relationship between features and outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can simply use correlation matrix of the features to discover any multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')\n",
    "# 'RdBu_r' & 'BrBG' are other good diverging colormaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Several coefficients are correlated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if error terms are uncorrelated as they should be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another option is to check the autocorrelation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "acf_data = acf(errors)\n",
    "\n",
    "plt.plot(acf_data[1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have high multicollinearity not supporting the OLS assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the features are correlated with the errors. A high correlation violates the OLS assumption (exogeneity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This assumption, is arguably the most important one: explanatory variables and errors should be independent. If this assumption doesn't hold, then the model's predictions will be unreliable as the estimates of the coefficients would be biased. This assumption is known as the exogeneity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert errors to series\n",
    "error_series = pd.Series(errors)\n",
    "X_copy = X.copy()\n",
    "X_copy['errors'] = error_series\n",
    "corr = X_copy.corr()\n",
    "corr.style.background_gradient(cmap='BuPu')\n",
    "# 'RdBu_r' & 'BrBG' are other good diverging colormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normality of residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When this assumption is violated, it causes problems with calculating confidence intervals and various significance tests for coefficients. When the error distribution significantly departs from Gaussian, confidence intervals may be too wide or too narrow. Although it is not an assumption of OLS, it still can impact our results. Specifically, normality of errors is not required to apply OLS to a linear regression model, but in order to measure the statistical significance of our estimated coefficients, error terms must be normally distributed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  An informal way of doing this is by visualizing the errors in a QQ plot or to look at the histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_nums = np.random.normal(np.mean(errors), np.std(errors), len(errors))\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(np.sort(rand_nums), np.sort(errors)) # we sort the arrays\n",
    "plt.xlabel(\"the normally distributed random variable\")\n",
    "plt.ylabel(\"errors of the model\")\n",
    "plt.title(\"QQ plot\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(errors)\n",
    "plt.xlabel(\"errors\")\n",
    "plt.title(\"Histogram of the errors\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above shows that our erros are almost normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Jargue Berra and normals tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import jarque_bera\n",
    "from scipy.stats import normaltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jb_stats = jarque_bera(errors)\n",
    "norm_stats = normaltest(errors)\n",
    "\n",
    "print(\"Jarque-Bera test statistics is {0} and p value is {1}\".format(jb_stats[0], jb_stats[1]))\n",
    "print(\"Normality test statistics is {0} and p value is {1}\".format(norm_stats[0], norm_stats[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The p-values of both tests are zero indicating that our errors are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Y is the target variable\n",
    "Y = df5['list_price']\n",
    "# X is the feature set which includes\n",
    "numerics = ['int_', 'int8', 'uint8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "print(numerics)\n",
    "df5_num = df5.select_dtypes(include=numerics)\n",
    "print(df5_num.shape)\n",
    "X = df5_num.drop(['list_price', 'close_price', 'target'], axis=1)\n",
    "X = sm.add_constant(X)\n",
    "# We fit an OLS model using statsmodels\n",
    "results = sm.OLS(Y, X).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most of the p-values of coefficients are close to zero indicating they are statistically significants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.copy()\n",
    "print('The shapes of dataset as we proceed: {}, {}, {}, {}, {}'.format(df0.shape, df1.shape, df4.shape, df5.shape, df6.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add dummy variables for some categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tcat_lst = [\n",
    "    #\n",
    "    #'street_name','city_location', 'style'\\\n",
    "    #'subdivision'\n",
    "    #, 'market_area'\\\n",
    "    #,'school_district', 'school_elementary', 'school_middle', 'school_high', \n",
    "    #,'list_agent_mlsid' \\\n",
    "    #,'list_office_mlsid'\\\n",
    "    #'selling_office_mlsid', 'close_date', 'year_built' \\\n",
    "    #'zip_code'\n",
    "    #'area'\n",
    "    'county'\n",
    "    ]\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(data= df6, columns=cat_lst)\n",
    "df7 = dummies.copy()\n",
    "print('The shapes of dataset as we proceed: {}, {}, {}, {}, {}, {}'.format(df0.shape, df1.shape, df4.shape, df5.shape, df6.shape, df7.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df7[df7.duplicated('import_id')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_to_left_get_dup(df7, 'import_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shapes of dataset as we proceed: {}, {}, {}, {}, {}, {}'.format(df0.shape, df1.shape, df4.shape, df5.shape, df6.shape, df7.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot2 = pd.pivot_table(df7,index=[\"target\",\"zip_code\"],aggfunc=[len], margins=True).round(0)\n",
    "\n",
    "#pivot2.columns\n",
    "#pivot2.sort_values(by=('len', 'target'), ascending=False)\n",
    "#pivot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y is the target variable\n",
    "Y = df7['list_price']\n",
    "\n",
    "# X is the feature set which includes\n",
    "numerics = ['int_', 'int8', 'uint8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "df7_num = df7.select_dtypes(include=numerics)\n",
    "X = df7_num.drop(['list_price', 'close_price', 'target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#new_X = pd.DataFrame(mca.transform(X), columns=['mca%i' % i for i in range(n_components)], index=df7.index)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 465)\n",
    "print(\"df7.shape:{}, X.shape:{}, Y.shape:{}, X_train.shape:{}, X_test.shape:{}, y_train.shape:{}, y_test.shape{}\".format(df7.shape, X.shape, Y.shape, X_train.shape, X_test.shape, y_train.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alphas = np.array([1000000, 1000, 10,  1, 0.1, 0.01, 0.001])\n",
    "model = Ridge(fit_intercept = True)\n",
    "grid = GridSearchCV(estimator=model, cv = 5, param_grid=dict(alpha=alphas))\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Ridge\n",
    "regression = Ridge(alpha=1)\n",
    "regression.fit(X_train, y_train)\n",
    "y_pred = regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the regression result performance\n",
    "print(\"--------------Ridge--------------------\")\n",
    "print(\"------- Number of Observation ---------\")\n",
    "print(\"The number of observations in training set is {}\".format(X_train.shape[0]))\n",
    "print(\"The number of observations in test set is {}\".format(X_test.shape[0]))\n",
    "print(\"----------------------------------------\")\n",
    "print(\"----- Performance on training data -----\")\n",
    "regression.fit(X_train, y_train)\n",
    "y_pred = regression.predict(X_train)\n",
    "regression_results(y_train, y_pred)\n",
    "print(\"----------------------------------------\")\n",
    "print(\"----- Performance on test data -----\")\n",
    "regression.fit(X_train, y_train)\n",
    "y_pred = regression.predict(X_test)\n",
    "regression_results(y_test, y_pred)\n",
    "print(\"----------------------------------------\")\n",
    "print(\"----- Coefficients and intercept data -----\")\n",
    "print(regression.coef_)\n",
    "#print(regression.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alphas = np.array([1000000, 100000, 1000, 1, 0.1, 0.01, 0.001])\n",
    "model = Lasso(fit_intercept = True)\n",
    "grid = GridSearchCV(estimator=model, cv=5, param_grid=dict(alpha=alphas))\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Lasso\n",
    "regression = Lasso(alpha=1)\n",
    "regression.fit(X_train, y_train)\n",
    "y_pred = regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the regression result performance\n",
    "print(\"-------------Lasso----------------------\")\n",
    "print(\"------- Number of Observation ---------\")\n",
    "print(\"The number of observations in training set is {}\".format(X_train.shape[0]))\n",
    "print(\"The number of observations in test set is {}\".format(X_test.shape[0]))\n",
    "print(\"----------------------------------------\")\n",
    "print(\"----- Performance on training data -----\")\n",
    "regression.fit(X_train, y_train)\n",
    "y_pred = regression.predict(X_train)\n",
    "regression_results(y_train, y_pred)\n",
    "print(\"----------------------------------------\")\n",
    "print(\"----- Performance on test data -----\")\n",
    "regression.fit(X_train, y_train)\n",
    "y_pred = regression.predict(X_test)\n",
    "regression_results(y_test, y_pred)\n",
    "print(\"----------------------------------------\")\n",
    "print(\"----- Coefficients and intercept data -----\")\n",
    "print(regression.coef_)\n",
    "print(regression.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet regression  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#alphas = np.array([1000000, 100000, 1000, 1, 0.1, 0.01, 0.001])\n",
    "param_grid = {\"max_iter\": [1, 5, 10], \"alpha\":[1000000, 100000, 1000, 1, 0.1, 0.01, 0.001], \"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n",
    "\n",
    "model = ElasticNet(fit_intercept = True)\n",
    "grid = GridSearchCV(estimator=model, cv = 5, param_grid=param_grid)#) #dict(alpha=alphas))\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.max_iter)\n",
    "print(grid.best_estimator_.alpha)\n",
    "print(grid.best_estimator_.l1_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ElasticNet\n",
    "regression = ElasticNet(max_iter=5, alpha=0.001, l1_ratio=0.7)\n",
    "regression.fit(X_train, y_train)\n",
    "y_pred = regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the regression result performance\n",
    "print(\"-----------ElasticNet------------------\")\n",
    "print(\"------- Number of Observation ---------\")\n",
    "print(\"The number of observations in training set is {}\".format(X_train.shape[0]))\n",
    "print(\"The number of observations in test set is {}\".format(X_test.shape[0]))\n",
    "print(\"----------------------------------------\")\n",
    "print(\"----- Performance on training data -----\")\n",
    "regression.fit(X_train, y_train)\n",
    "y_pred = regression.predict(X_train)\n",
    "regression_results(y_train, y_pred)\n",
    "print(\"----------------------------------------\")\n",
    "print(\"----- Performance on test data -----\")\n",
    "regression.fit(X_train, y_train)\n",
    "y_pred = regression.predict(X_test)\n",
    "regression_results(y_test, y_pred)\n",
    "print(\"----------------------------------------\")\n",
    "print(\"----- Coefficients and intercept data -----\")\n",
    "#print(regression.coef_)\n",
    "#print(regression.intercept_)"
   ]
  },
  {
   "source": [
    "## Random Forest Regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 2)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 50, num = 2)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rfr = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "rf_random = RandomizedSearchCV(estimator = rfr, param_distributions = random_grid, n_iter = 1, cv = 3, verbose=2, random_state=42, n_jobs = 8 )#-1 means all cores are used)\n",
    "#Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To determine if random search yielded a better model, we compare the base model with the best random search model.\n",
    "def evaluate(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    print(predictions.shape)\n",
    "    y_test = y_test.values\n",
    "    errors = abs(predictions - y_test)\n",
    "    mape = 100 * np.mean(errors / y_test)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "source": [
    "## Random search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 100, 120],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'n_estimators': [1200, 1400, 1600]\n",
    "}\n",
    "# Create a based model\n",
    "rfr = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rfr, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = 5, verbose = 2)\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "source": [
    "## It seems we have about maxed out performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the models perform poorly. This require additional feature engineering "
   ]
  },
  {
   "source": [
    "## Pick one of fited model for market price prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Ridge\n",
    "ridge_regression = Ridge(alpha=1)\n",
    "ridge_regression.fit(X_train, y_train)\n",
    "y_pred = ridge_regression.predict(X)\n",
    "\n",
    "# Create temporary pandas to combine (left join) original dataset with new market price prediction\n",
    "X1 = X.copy()\n",
    "X1['value_predicted'] = y_pred\n",
    "X1.drop(X1.columns.difference(['import_id','value_predicted']), 1, inplace=True)\n",
    "df8 = pd.merge(left=df7, right=X1, how='left', left_on='import_id', right_on='import_id')\n",
    "print(\"df7.shape: {}, df8.shape: {}, y_pred.shape:{}, X_train.shape:{}, y_train.shape:{}\".format(df7.shape, df8.shape, y_pred.shape, X_train.shape, y_train.shape))\n",
    "del X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_to_left_get_dup(df8, 'import_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distict_count(df, col_name):\n",
    "    #df_empty = pd.DataFrame({'A' : []})\n",
    "    df_empty = df.groupby([col_name]).size()\n",
    "    return df_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Calculate how much listing price deviates from the expected value\n",
    "\n",
    "df8['value_price_delta'] = df8['value_predicted'] - df8['list_price']\n",
    "df8['value_price_delta_percent'] = (df8['value_price_delta']/df8['list_price'])*100 \n",
    "df8['value_price_delta_percent'] = df8['value_price_delta_percent']#a.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int_', 'int8', 'uint8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "df8_num = df8.select_dtypes(include=numerics)\n",
    "df8_num.columns\n",
    "Y = df8['target']\n",
    "X = df8_num.drop(['target', 'close_price'], axis=1)"
   ]
  },
  {
   "source": [
    "## Check the balance of the target features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.shape, X.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.35, random_state = 465)\n",
    "print(\"X.shape:{}, Y.shape:{}, X_train.shape:{}, X_test.shape:{}, y_train.shape:{}, y_test.shape{}\".format(X.shape, Y.shape, X_train.shape, X_test.shape, y_train.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_with_nulls(df8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#iris = datasets.load_iris()\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "    ])\n",
    "\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pca = PCA(n_components=None, svd_solver='randomized', whiten=True, random_state=42)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "model = make_pipeline(pca, svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.35, random_state = 465)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "\n",
    "%time grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "yfit = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, yfit))#)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "mat = confusion_matrix(y_test, yfit)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n",
    "            cmap='Blues', cbar=False,\n",
    "            xticklabels= df8.target,\n",
    "            yticklabels= df8.target)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}